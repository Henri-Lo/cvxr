---
title: "Introduction to CVXR!"
author: "Stephen Boyd, Steven Diamond, Anqi Fu, Balasubramanian Narasimhan, Paul Rosenfield"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
    fig_caption: yes
vignette: >
  %\VignetteIndexEntry{Introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, message=FALSE, warning=FALSE}
library(MASS)
library(ggplot2)
library(Rmisc)
library(cvxr)
```

# 1 Introduction

Welcome to `CVXR`: a modeling language for describing and solving convex optimization problems that follows the natural, mathematical notation of convex optimization rather than the requirements of any particular solver. The purpose of this document is both to introduce the reader to `CVXR` and to generate excitement its possibilities in the field of statistics.

Convex optimization is a powerful and very general tool. As a practical matter, the set of convex optimization problems includes almost every optimization problem that can be solved exactly and efficiently (i.e. without requiring an exhaustive search). If an optimization problem can be solved, it is probably convex. This family of problems becomes even larger if you include those that can be solved _approximately_ and efficiently. To learn more about the mathematics and application of convex optimization, see [Boyd and Vandenberghe 2009](http://stanford.edu/~boyd/cvxbook/).

Convex optimization systems written in other languages are already widely used in practical applications. These include [YALMIP](http://users.isy.liu.se/johanl/yalmip/pmwiki.php?n=Main.WhatIsYALMIP) and [CVX](http://cvxr.com/cvx/) (Matlab), [CVXPY](http://www.cvxpy.org/en/latest/) (Python), and [Convex.jl](http://convexjl.readthedocs.org/en/latest/) (Julia). `CVXR` Shares a lot of its code base with [CVXcannon](http://stanford.edu/class/ee364b/projects/2015projects/reports/miller_quigley_zhu_report.pdf) and CVXPY. As far as we know, this is the first full-featured general convex optimization package for R.

One of the great headaches of conventional numerical optimization is the process of deciding which algorithm to use and how to set its parameters. In convex optimization, the particular algorithm matters much less. So while a user of `CVXR` is still free to choose from a number of different algorithms and to set algorithm parameters as they please, the vast majority of users will not need to do this. `CVXR` will just work.

The uses for convex optimization in statistics are many and varied. Many parameter-fitting methods are convex, including least-squares, ridge, lasso, and isotonic regression, as well as many other kinds of problems such as maximum entropy or minimum Kullback-Leibler divergence over a finite set.

All of these examples, at least in their most basic forms, are established enough that they already have well-crafted R packages devoted to them. If you use `CVXR` to solve these problems, it will work. It will probably be slower than a custom-built algorithm---for example glmnet for fitting lasso or ridge regression models---but it will work. However, this is not the true purpose of `CVXR`. If you want to build a well-established model, you should use one of the well-established packages for doing so. If you want to build your _own_ model---one that is a refinement of an existing method, or perhaps even something that has never been tried before---then `CVXR` is the place to do it. The advantage of `CVXR` over glmnet and the like comes from its flexibility: A few lines of code can transform a problem from commonplace to state-of-the-art, and can often do the work of an entire package in the process. (We present an example in section 3 where this is literally the case.)

This document is meant to be a complete introduction to the `CVXR` package. It assumes basic knowledge of convex optimization and statistics as well as proficiency with R. A potential user of `CVXR` should read all of it, but especially sections 2 and 4. Section 5 can be skimmed and then used as a reference when necessary. The sections may be summarized as follows:

> Section 2: Thoroughly describes the most important aspects of `CVXR` for a new user to get started with a simple example.

> Section 3: Presents a sequence of more sophisticated examples which are meant to illustrate the ease with which `CVXR` can handle a diverse set of problems.

> Section 4: While the family of convex optimization problems is very large, it nonetheless has severe limitations which you must understand in order to be an effective user of `CVXR`. This section lays out these limitations and how they are handled through a system called Disciplined Convex Programming (DCP).

> Section 5: Presents the complete set of functions that have been implemented in `CVXR` so far along with some of their most essential properties.

Happy optimizing!


# 2 Convex Optimization

A convex optimization problem has the following form:
$$
\begin{array}{ll} \mbox{minimize} & f_0(x)\\
	\mbox{subject to} & f_i(x) \leq 0, \quad i=1,\ldots,m\\
	& g_i(x) = 0, \quad i=1,\ldots,p
	\end{array}
$$
where $x$ is the variable, $f_0$ and $f_1,...,f_m$ are convex and $g_1,...,g_p$ are affine. $f_0$ is called the objective function, $f_i \leq 0$ are called the inequality constraints, and $g_i = 0$ are called the equality constraints.

A convex function ...

A concave function...

An affine function...

In CVXR, you will specify convex optimization problems in a more convenient format than the one above.

A convex function is one that is upward curving. A concave function is downward curving. An affine function is flat, and is thus both convex and concave.

A convex optimization problem is one that attempts to minimize a convex function (or maximize a concave function) over a convex set of input points. 

Explain intuitively why these two properties make convex functions uniquely tractable.

You can learn much more about convex optimization by [book, ee364A, MOOC CVX101, convex optimization short course]

 [Boyd and Vandenberghe 2009](http://stanford.edu/~boyd/cvxbook/)



# 3 'Hello World'

We begin with one of the simplest possible problems that presents all three of these features:
$$
\begin{array}{ll}
  \mbox{minimize} & x^2 + y^2 \\
	\mbox{subject to} & x \geq 0, \quad 2x + y = 1
\end{array}
$$
with scalar variables $x$ and $y$. This is a convex optimization problem with objective $f_0(x,y) = x^2 + y^2$ and constraint functions $f_1(x,y) = -x$ and $g_1(x,y) = 2x - y - 1$.

Note that this problem is simple enough to be solved analytically, so we can confirm that `CVXR` has produced the correct answer. Here's how we formulate the problem in `CVXR`.

```{r}
# Variables minimized over
x <- Variable(1)
y <- Variable(1)

# Problem definition
objective <- Minimize(x^2 + y^2)
constraints <- list(x >= 0, 2*x + y == 1)
prob2.1 <- Problem(objective, constraints)

# Problem solution
solution2.1 <- solve(prob2.1)
solution2.1$status
solution2.1$optimal_value
solution2.1$primal_values[[as.character(x@id)]]
solution2.1$primal_values[[as.character(y@id)]]

# The world says 'hi' back.
```

We now turn to a careful explanation of the code. The first lines create two Variable objects, `x` and `y`, both of length 1 (i.e. scalar variables).

```{r}
x <- Variable(1)
y <- Variable(1)
```

`x` and `y` represent what we are allowed to adjust in our problem in order to obtain the optimal solution. They don't have values yet, and they won't until after we solve the problem. For now, they are just placeholders.

Next, we define the problem objective.

```{r}
objective <- Minimize(x^2 + y^2)
```

This call to `Minimize()` does _not_ return the minimum value of the expression `x^2 + y^2` the way a call to the native R function `min()` would do (after all, `x` and `y` don't have values yet). Instead, `Minimize()` creates an Objective object, which defines the goal of the optimization we will perform, namely to find values for `x` and `y` which produce the smallest possible value of `x^2 + y^2`.

The next line defines two constraints---an inequality constraint and an equality constraint, respectively. 

```{r}
constraints <- list(x >= 0, 2*x + y == 1)
```

Again, counter to what you might ordinarily expect, the expression `x >= 0` does not return `TRUE` or `FALSE` the way `1.3 >= 0` would. Instead, the `==` and `>=` operators have been overloaded to return Constraint objects which will be used by the solver to enforce the problem's constraints. (Without them, the solution to our problem would simply be $x = y = 0$.)

Next, we define our Problem object, which takes our Objective object and our two Constraint objects as inputs.

```{r}
prob2.1 <- Problem(objective, constraints)
```

Problem objects are very flexible in that they can have 0 or more Constraints, and their Objective can be to `Minimize()` a convex expression (as shown above) _or_ to `Maximize()` a concave expression. You can even create a Problem object with no Objective, in which case the corresponding problem is called a "feasibility problem," where the implicit objective is simply to find out whether or not it is possible to simultaneously satisfy all of the given constraints.

The call to `Problem()` still does not actually _solve_ our optimization problem. That only happens with the call to `Solve()`. 

```{r}
solution2.1 <- solve(prob2.1)
```

Behind the scenes, this call translates the problem into a format that a convex solver can understand, feeds the problem to the solver, and then returns the results to a Solution object.  For this problem, the Solution object will contain among other things the optimal value of the objective function `x^2 + y^2`, values for `x` and `y` that achieve that optimal objective value, and some accompanying metadata such as `solution2.1$status`, which confirms that the solution was indeed `"optimal"`.

```{r}
solution2.1$status
solution2.1$optimal_value
solution2.1$primal_values[[as.character(x@id)]]
solution2.1$primal_values[[as.character(y@id)]]
```

In general when you apply the `Solve()` method to a Problem, several things can happen:

1. `solution$status == "optimal"`: The problem is solved. Values for the optimization variables are found which satisfy all of the constraints and minimize the objective.

2. `solution$status == "infeasible"`: The problem was _not_ solved because no combination of input variables exists that can satisfy all of the constraints. For a trivial example of when this might happen, consider a problem with optimization variable `x`, and constraints `x >= 1` and `x <= 0`. Obviously, no value of `x` exists that can satisfy both constraints. In this case, `solution$opt.val` is `+Inf` for a minimization problem and `-Inf` for a maximization problem, indicating infinite dissatisfaction with the result. No values are returned for the input variables.

3. `solution$status == "unbounded"`: The problem was _not_ solved because the Objective can be made arbitrarily small for a minimization problem or arbitrarily large for a maximization problem. Hence there is no optimal solution because for any given solution it is always possible to find something even more optimal. In this case, `solution$opt.val` is `-Inf` for a minimization problem and `+Inf` for a maximization problem, indicating infinite satisfaction with the result. Again, no values are returned for the input variables.

## 2.2 Modifying a CVXR Problem

Like any normal R object, the Problem, Objective, Constraint, and Solution objects can all be modified and computed on after creation. Here is an example where we modify the problem we created above by changing its objective and adding a constraint, print the modified problem, check whether it is still convex, and then solve the modified problem:

```{r}
# Modify the problem from example 1
prob2.2 <- prob2.1
prob2.2@objective <- Minimize(x^2 + y^2 + abs(x-y))
prob2.2@constraints <- c(prob2.2@constraints, y <= 1)

# Analyze the modified problem
print(prob2.2)
is_convex(prob2.2)

# Solve the modified problem
solution2.2 <- solve(prob2.2)

# Examine the solution
solution2.2$status
solution2.2$optimal_value
solution2.2$primal_values[[as.character(x@id)]]   # TODO: Make a R6 function for retrieving primal values cleanly
solution2.2$primal_values[[as.character(y@id)]]
```

## 2.3 An Invalid Problem

Unfortunately, you can't just type any arbitrary problem you like into `CVXR`. There are severe restrictions on what kinds of problems can be handled. For example, if we tried to `Maximize()' the objective from example 2.1, we get an error:

```{r}
prob2.3 <- prob2.1
prob2.3@objective <- Maximize(x^2 + y^2)
solve(prob2.3)
```

We would get a similar error if we tried to add the constraint `Norm2(x) == 1`. This is because `CVXR` uses a strict set of rules called Disciplined Convex Programming (DCP) to evaluate the convexity of any given problem. If you follow these rules, you are guaranteed that your problem is convex. If you don't follow these rules, `CVXR` will throw an exception. See section 5 for a complete description of DCP.

# 4 Examples

We begin by showing what a standard linear regression problem looks like in `CVXR`:

## 3.1 Ordinary Least Squares
```{r, echo=FALSE}
set.seed(1)
s <- 1
m <- 10
n <- 300
mu <- rep(0, 9)
Sigma <- data.frame(c(1.6484, -0.2096, -0.0771, -0.4088, 0.0678, -0.6337, 0.9720, -1.2158, -1.3219),
                    c(-0.2096, 1.9274, 0.7059, 1.3051, 0.4479, 0.7384, -0.6342, 1.4291, -0.4723),
                    c(-0.0771, 0.7059, 2.5503, 0.9047, 0.9280, 0.0566, -2.5292, 0.4776, -0.4552),
                    c(-0.4088, 1.3051, 0.9047, 2.7638, 0.7607, 1.2465, -1.8116, 2.0076, -0.3377),
                    c(0.0678, 0.4479, 0.9280, 0.7607, 3.8453, -0.2098, -2.0078, -0.1715, -0.3952),
                    c(-0.6337, 0.7384, 0.0566, 1.2465, -0.2098, 2.0432, -1.0666,  1.7536, -0.1845),
                    c(0.9720, -0.6342, -2.5292, -1.8116, -2.0078, -1.0666, 4.0882,  -1.3587, 0.7287),
                    c(-1.2158, 1.4291, 0.4776, 2.0076, -0.1715, 1.7536, -1.3587, 2.8789, 0.4094),
                    c(-1.3219, -0.4723, -0.4552, -0.3377, -0.3952, -0.1845, 0.7287, 0.4094, 4.8406))

X <- mvrnorm(n, mu, Sigma)
X <- cbind(rep(1, n), X)
trueBeta <- c(0, 0.8, 0, 1, 0.2, 0, 0.4, 1, 0, 0.7)
y <- X %*% trueBeta + rnorm(n, 0, s)
```

```{r}
beta <- Variable(m)
objective <- Minimize(SumSquares(y - X %*% beta))
prob3.1 <- Problem(objective)
```

Here, `y` is the response, `X` is the matrix of predictors, `n` is the number of observations, and `beta` is a vector of coefficients on the predictors. The Ordinary Least-Squares (OLS) solution for `beta` minimizes `1/n` times the the $l_2$-norm of the residuals (i.e. the root-mean-squared error). As we can see below, `CVXR`'s solution matches the solution obtained by using `lm`.

```{r}
CVXR_solution3.1 <- solve(prob3.1)
lm_solution3.1 <- lm(y ~ 0 + X)
```

```{r, echo=FALSE, fig.height = 5, fig.width = 7}
lmBeta <- coef(lm_solution3.1)
cvxrBeta <- CVXR_solution3.1$primal_values[[as.character(beta@id)]]
coefs <- data.frame( estimate = c(lmBeta, cvxrBeta),
                     method = c(rep("lm", 10), rep("CVXR", 10)),
                     beta = rep(paste("beta", 0:9, sep = ""), 2))
ggplot(data = coefs, aes(x = as.factor(beta), y = estimate, fill = method)) + 
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Comparison of CVXR to lm", x = "coefficient", y = "")
```

Obviously, if all you want to do is least-squares linear regression, you should simply use `lm`. The chief advantage of `CVXR` is its flexibility, as we will demonstrate with the rest of section 3.

## 3.2 Non-Negative Least Squares

Looking at Example 3.1, you may notice that the OLS regression problem has an objective, but no constraints. In many contexts, we can greatly improve our model by constraining the solution to reflect our prior knowledge. For example, we may know that the coefficients `beta` must be non-negative.

```{r}
prob3.2 <- prob3.1
prob3.2@constraints <- list(beta >= 0)
solution3.2 <- solve(prob3.2)
```

```{r, echo=FALSE, out.width=700}
cvxrBetaNNLS <- solution3.2$primal_values[[as.character(beta@id)]]
coefsNNLS <- data.frame ( estimate = c(trueBeta, lmBeta, cvxrBetaNNLS),
                          method = c(rep("Actual", 10), rep("OLS", 10), rep("NNLS", 10)),
                          beta = rep(paste("beta", 0:9, sep = ""), 3))
ggplot(data = coefsNNLS, aes(x = factor(beta), y = estimate, fill = method)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Least Squares Coefficients", x = "coefficient", y = "")
```

As we can see in the figure above, adding that one constraint produced a massive improvement in the accuracy of the estimates. Not only are the non-negative least-squares estimates much closer to the true signal than the OLS estimates, they have even managed to recover the correct sparsity structure in this case.

Like with OLS, there are already R packages available which implement non-negative least squares, such as the package `nnls`. But that is actually an excellent demonstration of the power of `CVXR`: A single line of code here, namely `prob3.2$constraints <- list(beta >= 0)`, is doing the work of an entire package.


## 3.3 Support Vector Classifiers

Another common statistical tool is the support vector classifier (SVC). The SVC is an affine function (hyperplane) that separates two sets of points by the widest margin. When the sets are not linearly separable, the SVC is determined by a trade-off between the width of the margin and the number of points that are misclassified.

For the binary case, where the response $y_i \in \{-1,1\}$, the SVC is obtained by solving
$$
\begin{array}{ll}
  \mbox{minimize} & \frac{1}{2}\Vert\beta\Vert^2 + C\sum_{i=1}^m \xi_i  \\
	\mbox{subject to} & \xi_i \geq 0, \quad y_i(x_i^T\beta + \beta_0) \geq 1 - \xi_i, \quad i = 1,\ldots,m
\end{array}
$$
with variables $(\beta,\xi)$. Here, $\xi$ is the amount by which a point can violate the separating hyperplane, and $C > 0$ is a user-chosen penalty on the total violation. As $C$ increases, fewer misclassifications will be allowed. 

Below, we fit a SVC in `CVXR` with $C = 10$.

```{r, echo=FALSE}
# Generate data
set.seed(10)
n <- 2
m <- 50

X <- matrix(rnorm(m*n), nrow = m, ncol = n)
y <- c(rep(-1, m/2), rep(1, m/2))
X[y == 1,] = X[y == 1,] + 1
```

```{r}
# Define variables
cost <- 10
beta0 <- Variable()
beta <- Variable(n)
slack <- Variable(m)

# Form problem
objective <- (1/2)*SumSquares(VStack(beta, beta0)) + cost*sum(slack)
constraints <- list(y * (X %*% beta + beta0) >= 1 - slack, slack >= 0)
prob3.3 <- Problem(Minimize(objective), constraints)
solution3.3 <- solve(prob3.3)
```

```{r, echo=FALSE, fig.width = 7}
b0 <- solution3.3$primal_values[[as.character(beta0@id)]]
b <- solution3.3$primal_values[[as.character(beta@id)]]

# Plot support vector and classified points
plotdata <- data.frame(cbind(y, X))
colnames(plotdata) <- c("y", "X1", "X2")
ggplot(data = plotdata, aes(x = X1, y = X2)) +
  geom_point(size = 3, color = factor(3-plotdata$y)) +
  geom_abline(slope = -b[1]/b[2], intercept = (1 - b0)/b[2]) +
  labs(title = "Binary Support Vector Classifier")
```

## 3.4 Direct Standardization -- Correcting Bias in a Sample

Suppose you have a population you want to study, such as people eligible to vote in a US presidential primary, but you do not have a good way of drawing a simple random sample from that population. Instead, your sampling method is biased to oversample some subgroups of the population and undersample others. Ideally, you would like to take this sampling bias into account before conducting any kind of analysis on your sample data. Direct standardization offers one approach for dealing with this problem.

Direct standardization works by assigning a weight to each observation in your sample. The weights are chosen to maximize the entropy of the weighted empirical distribution subject to a series of constraints reflecting your prior knowledge of the population. In effect, this puts less weight on observations from oversampled subgroups and more weight on observations from undersampled subgroups.

We begin by generating our population data consisting of a feature matrix $X$ and vector $y$ correlated with the columns of $X$. Then, we draw an extremely biased sample $(\tilde y, \tilde X)$ from it. From the plot below, we observe that smaller values in $y$ are overrepresented in the sample.

```{r, echo=FALSE, warning=FALSE, fig.width = 7}
data(dspop)    # Population
data(dssamp)   # Biased sample

plotdata <- rbind(dspop, dssamp)
plotdata$sample <- as.factor(c(rep("population", nrow(population)), rep("sample", nrow(sample))))

ggplot(data = plotdata, aes(x = y, color = sample)) +
  geom_density(size = 1.5) + 
  scale_color_manual("", values = c("#F8766D", "#619CFF")) +
  labs(title = "Probability Density of Population and Biased Sample")
```

We will attempt to correct this bias in the sample distribution (the blue curve) based on limited prior knowledge of the population distribution (the red curve) using direct standardization. In this case, our prior knowledge is the population mean of the features, given as a vector $b$. We solve the problem

$$
\begin{array}{ll}
  \mbox{maximize} & \sum_i^m -w_i \log w_i \\
	\mbox{subject to} & w \geq 0, \quad \mathbf{1}^Tw = 1, \quad \tilde X^Tw = b
\end{array}
$$
Here is how we implement that with `CVXR`:

```{r, echo=FALSE}
# Given population mean of features
X <- dspop[,-1]
Xsamp <- dssamp[,-1]
b <- as.matrix(apply(X, 2, mean))
```

```{r}
# Problem definition
w <- Variable(nrow(Xsamp))
objective <- sum(Entr(w))
constraints <- list(w >= 0, sum(w) == 1, t(Xsamp) %*% w == b)
prob3.4 <- Problem(Maximize(objective), constraints)

# Solve for the distribution weights
solution3.4 <- solve(prob3.4)
```

As you can see in the figure below, the resulting standardized sample is quite close to the population distribution.

```{r, echo=FALSE, warning=FALSE, fig.width = 7}
weights <- solution3.4$primal_values[[as.character(w@id)]]
plotdata2 <- rbind(dspop, dssamp, dssamp)
plotdata2$sample <- as.factor(c(rep("population", nrow(dspop)),
                                rep("sample", nrow(dssamp)),
                                rep("standardized", nrow(dssamp))))
plotdata2$p <- c(rep(1/nrow(dspop), nrow(dspop)),
                 rep(1/nrow(dssamp), nrow(dssamp)),
                 weights)

ggplot(data = plotdata2, aes(x = y, color = sample, weight = p)) +
  geom_density(size = 1.5) + 
  scale_color_manual("", values = c("#F8766D", "#619CFF", "#00BA38")) +
  labs(title = "Population, Biased Sample, and Standardized Sample")
```


## 3.5 Total Variation Inpainting

Often in imaging applications, portions of a picture or video may be lost due to physical deterioration, transmission errors, or issues with the source itself. An important challenge in signal processing is the recovery of this lost data, otherwise known as inpainting. Given a corrupt image, we would like to reconstruct the original based on the known pixels and our general understanding of visual systems.

One approach to this problem is via convex optimization. We represent an image as a matrix of pixel values $U \in \mathbf{R}^{m \times n}$. Then, we minimize the total variation of $U$ subject to it matching the known pixels in our given image. We use the $l2$-norm based total variation,
$$
\mathbf{tv}(U) = \sum_{i=1}^{m-1} \sum_{j=1}^{n-1} \left\Vert\left[\begin{array}{c}
  U_{i+1,j} - U_{ij} \\
  U_{i,j+1} - U_{ij}
\end{array}\right]\right\Vert_2
$$
which measures the degree of fluctuation in a matrix. This function is employed in a variety of image processing tasks, such as reconstruction, denoising, and deblurring, because of its ability to preserve sharp edges.

For our example, we are given a corrupted image of Lena in which certain pixels have been overwritten.

```{r, echo=FALSE, fig.width = 7}
data(lena)         # True
data(lena_corr)    # Corrupted
rows <- nrow(lena)
cols <- ncol(lena)

# Display image
image(lena_corr, main = "Corrupted Image", axes = FALSE, col = grey(seq(0, 1, length = 256)))

# Indicator of known pixels
Known <- matrix(0, nrow = rows, ncol = cols)
Known[lena == lena_corr] <- 1
```

The indices of these pixels are represented as an indicator matrix $K$, where $K_{ij} = 0$ if the pixel is lost and 1 otherwise. The problem formulation in `CVXR` looks like

```{r}
# Problem definition
U <- Variable(rows, cols)
objective <- Minimize(TotalVariation(U))

# Restrict known pixels with equality
constraints <- list(Known * U == Known * lena_corr)
prob3.5 <- Problem(objective, constraints)

# Reconstruct image
solution3.5 <- solve(prob3.5)
```

As you can see, the reconstructed image is visually very similar to the original:

```{r, echo=FALSE, fig.width = 7}
lena_recon <- solution3.5$primal_values[[as.character(U@id)]]
image(lena, main = "Original Image", axes = FALSE, col = grey(seq(0, 1, length = 256)))
image(lena_recon, main = "Reconstructed Image", axes = FALSE, col = grey(seq(0, 1, length = 256)))
```

# 4 Disciplined Convex Programming (DCP)

Disciplined convex programming (DCP) is a system for constructing mathematical expressions with known curvature from a given library of base functions. `CVXR` uses DCP to ensure that the specified optimization problems are convex.

This section explains the rules of DCP and how they are applied by `CVXR`. You may also find it helpful to read about the DCP rules are applied in other languages such as [Python](http://www.cvxpy.org/en/latest/tutorial/dcp/index.html),  [Matlab](http://cvxr.com/cvx/doc/dcp.html#), and [Julia](http://convexjl.readthedocs.io/en/latest/types.html).

ALSO LINK TO [dcp](www.cvxr.com/dcp/). EMAIL STEVEN DIAMOND ABOUT DCP.STANFORD.EDU NOT WORKING

## 4.1 Expressions

Expressions in `CVXR` are formed from variables, (parameters?), numerical constants such as numeric vectors and matrices, the standard arithmetic operators `+, -, *, /`, and a library of functions. For example,

```{r, eval=FALSE}

# Create variables and parameters.
x <- y <- Variable()
#a, b = Parameter(), Parameter() does CVXR have parameters? What are they?

# Examples of CVXPY expressions.
3.69 + b/3
x - 4*a
sqrt(x) - min(y, x - a)
max(2.66 - sqrt(y), (x + 2*y)^2)
```

Expressions can be scalars, vectors, or matrices. The dimensions of an expression are stored as `expr$size`. `CVXR` will raise an exception if an expression is used in a way that doesn't make sense given its dimensions, for example adding matrices of different size.

```{r, eval=FALSE}
X <- Variable(5, 4)
A <- matrix(1, nrow = 3, ncol = 5)

# Use dim to get the dimensions.
paste0("dimensions of X: ", size(X))
paste0("dimensions of sum_entries(X): ", size(sum(X)))
paste0("dimensions of A*X: ", size(A %*% X))
```

`CVXR` uses DCP analysis to determine the sign and curvature of each expression.

## 4.2 Sign

Each (sub)expression is flagged as *positive* (non-negative), *negative* (non-positive), *zero*, or *unknown*.

The signs of larger expressions are determined from the signs of their subexpressions. For example, the sign of the expression `expr1*expr2` is

* Zero if either expression has sign zero.

* Positive if `expr1` and `expr2` have the same (known) sign.

* Negative if `expr1` and `expr2` have opposite (known) signs.

* Unknown if either expression has unknown sign.

The sign given to an expression is always correct. But DCP sign analysis may flag an expression as unknown sign when the sign could be figured out through more complex analysis. For instance, `x*x` is positive but has unknown sign by the rules above.

`CVXR` determines the sign of constants by looking at their value. For scalar constants, this is straightforward. Vector and matrix constants with all positive (negative) entries are marked as positive (negative). Vector and matrix constants with both positive and negative entries are marked as unknown sign.

The sign of an expression is stored as `expr$sign`:

```{r, eval=FALSE}
x <- Variable()
a <- Parameter(sign="negative")
b <- c(1, -1)

paste0("sign of x: ", sign(x))
paste0("sign of a: ", sign(a))
paste0("sign of square(x): ", sign(x^2))
paste0("sign of c*a: ", sign(c*a))
```

## 4.3 Curvature

Each (sub)expression is flagged as one of the following curvatures (with respect to its variables) using the composition rules given in section 4.4:

Curvature     | Meaning
--------------|-----------
constant      | $f(x)$ independent of $x$
affine        | $f( \theta x + (1-\theta)y) = \theta f(x) + (1 - \theta) f(y), \forall x, y, \theta \in [0, 1]$
convex        | $f( \theta x + (1-\theta)y) \leq \theta f(x) + (1 - \theta) f(y), \forall x, y, \theta \in [0, 1]$
concave       | $f( \theta x + (1-\theta)y) \geq \theta f(x) + (1 - \theta) f(y), \forall x, y, \theta \in [0, 1]$
unknown       | DCP analysis cannot determine the curvature

As with sign analysis, the conclusion is always correct, but the simple analysis can flag expressions as unknown even when they are convex or concave. Note that any constant expression is also affine, and any affine expression is convex and concave.


## 4.4 General Composition Rule

DCP analysis is based on applying a general composition theorem from convex analysis to each (sub)expression.

$f(\text{expr}_1, \text{expr}_2,...,\text{expr}_n)$ is convex if $f$ is a convex function and for each $\text{expr}_i$ one of the following conditions holds:

* $f$ is increasing in argument $i$ and $\text{expr}_i$ is convex.

* $f$ is decreasing in argument $i$ and $\text{expr}_i$ is concave.

* $expr_i$ is affine or constant.

$f(\text{expr}_1, \text{expr}_2,...,\text{expr}_n)$ is concave if $f$ is a concave function and for each $\text{expr}_i$ one of the following conditions holds:

* $f$ is increasing in argument $i$ and $\text{expr}_i$ is concave.

* $f$ is decreasing in argument $i$ and $\text{expr}_i$ is convex.

* $\text{expr}_i$ is affine or constant.

$f(\text{expr}_1, \text{expr}_2,...,\text{expr}_n)$ is affine if $f$ is an affine function and each $\text{expr}_i$ is affine.

If none of the three rules apply, the expression $f(\text{expr}_1, \text{expr}_2,...,\text{expr}_n)$ is marked as having unknown curvature.

Whether a function is increasing or decreasing in an argument may depend on the sign of the argument. For instance, `square()` is increasing for positive arguments and decreasing for negative arguments.

The curvature of an expression is stored as `expr$curvature`:

```{r, eval=FALSE}
x = Variable()
a = Parameter(sign="positive")

paste0("curvature of x: ", curvature(x))
paste0("curvature of a: ", curvature(a))
paste0("curvature of x^2: ", curvature(x^2))
paste0("curvature of sqrt(x): ", curvature(sqrt(x)))
```

The infix operators `+, -, *, /` are treated exactly like functions. The infix operators `+` and `-` are affine, so the rules above are used to flag the curvature. For example, `expr1 + expr2` is flagged as convex if `expr1` and `expr2` are convex.

`expr1*expr2` is allowed only when one of the expressions is constant. If both expressions are non-constant, `CVXR` will raise an exception. `expr1/expr2` is allowed only when `expr2` is a scalar constant. The curvature rules above apply. For example, `expr1/expr2` is convex when `expr1` is concave and `expr2` is negative and constant. This is sometimes referred to as the No-Product Rule.


SHORT LIST OF COMMON EXAMPLES? sum, max, min,



## 4.6 Example 1

DCP analysis breaks expressions down into subexpressions. The tree visualization below shows how this works for the expression `2*square(x) + 3`. Each subexpression is shown in a blue box. We mark its curvature on the left and its sign on the right.

![](images\DCPex1.png)

The variable `x` has affine curvature and unknown sign. The `square` function is convex and non-monotone for arguments of unknown sign. It can take the affine expression `x` as an argument; the result `square(x)` is convex.

`2` is positive and constant. The `*` operator is positive and convex given that one input is positive convex and the other is positive constant. `3` is positive and constant, and the arithmetic operator `+` is affine and increasing, so the composition `2*square(x) + 3` is convex by the curvature rule for convex functions and positive because both of its inputs are positive.

## 4.7 Example 2

We'll walk through the application of the DCP rules to the expression `sqrt(1 + x^2)`.

![](images\DCPex2.png)

As in Example 1, `x^2` is convex and positive.

The arithmetic operator `+` is affine and increasing, so the composition `1 + x^2` is convex by the curvature rule for convex functions. The function `sqrt` is concave and increasing, which means it can only take a concave argument. Since `1 + x^2` is convex, `sqrt(1 + x^2)` violates the DCP rules and cannot be verified as convex.

In fact, `sqrt(1 + x^2)` is a convex function of $x$, but the DCP rules are not able to verify convexity. If the expression is written as `norm2(c(1, x))`, the l2 norm of the vector `[1] 1 2`, which has the same value as `sqrt(1 + x^2)`, then it will be certified as convex using the DCP rules.

```{r, eval=FALSE}
paste0("sqrt(1 + x^2) curvature: ", curvature(sqrt(1 + x^2)))
paste0("norm2(vstack(1, x)) curvature: ", curvature(norm2(c(1, x))))
```


## 4.9 Constraints

The list of supported convex constraints is

* *affine* `==` *affine*

* *convex* `<=` *concave*

* *concave* `>=` *convex*

Note that an affine expression is both convex and concave, so a constraint like

```{r, eval=FALSE}
list( abs(x) <= y ) )
```

is valid because `y` is affine and thus also concave. Non-equality `!=` is not supported because they are almost never convex.

IN A DCP PROBLEM, WE HAVE A LIST OF CONSTRAINTS. Example:


EXAMPLES: like norm(beta, 1) <= t


## 4.8 Objectives

min convex
max concave
min or max affine


## 4.9 Problems

A problem is formed with an objective and a list of constraints.

Problems are R objects. You can query them, etc.

Most important function is the Solve() function. Explain what it does.


# 5 The CVXR Function Library

This section describes the library of functions that can be applied to `CVXR` expressions. `CVXR` uses the function information in this section and the DCP rules to mark expressions with a sign and curvature.

## 5.1 Operators

The infix operators `+, -, *, /` are treated as functions. `+` and `-` are affine functions. `*` and `/` are affine in `CVXR` because `expr1*expr2` is allowed only when one of the expressions is constant and `expr1/expr2` is allowed only when `expr2` is a scalar constant.

## 5.2 Indexing and Slicing

All non-scalar expressions can be indexed using the syntax `expr[i, j]`. Indexing is an affine function. The syntax `expr[i]` can be used as a shorthand for `expr[i, 1]` when `expr` is a column vector. Similarly, `expr[i]` is shorthand for `expr[1, i]` when `expr` is a row vector.

Non-scalar expressions can also be sliced into using the standard `R` slicing syntax. For example, `expr[seq(i, j, k), r]` selects every `k`th element in column `r` of `expr`, starting at row `i` and ending at row `j`.

`CVXR` supports advanced indexing using lists of indices or boolean arrays. The semantics are the same as in base `R`.

## 5.3 Transpose

The transpose of any expression can be obtained using `t(expr)`. Transpose is an affine function.

## 5.4 Power

For any `CVXR` expression `expr`, the power operator `expr^p` is equivalent to the function `power(expr, p)`.

## 5.5 Scalar Functions


pos, neg, max, min, norm, square, sum_square, sqrt, log, exp, entr




A scalar function takes one or more scalars, vectors, or matrices as arguments and returns a scalar.

Function               | Meaning            | Domain   | Sign        | Curvature    
-----------------------|--------------------|----------|-------------|-----------
geo_mean(x) <br> <br> geo_mean(x, p) <br> <br> $p \in R_+^n$ <br> <br> $p \neq 0$ | $x_1^{1/n} \cdots x_n^{1/n}$ <br><br> $(x_1^{p_1} \cdots x_n^{p_n})^{\frac{1}{\mathbf{1}^T p}}$ |  $x \in \mathbf{R}_+^n$ |  positive |  concave
harmonic_mean(x) | $\frac{n}{\frac{1}{x_1} + \cdots \frac{1}{x_n}}$ | $x \in \mathbf{R}_+^n$ | positive | concave

The domain $\mathbf{S}_+^n$ and $\mathbf{S}_-^n$ refer to the set of positive semi-definite and negative semi-definite matrices, respectively. Similarly, $\mathbf{S}_{++}^n$ and $\mathbf{S}_{--}^n$ refer to the set of positive definite and negative definite matrices, respectively.



## 5.6 Functions along an Axis

## 5.7 Elementwise Functions

## 5.8 Vector/Matrix Functions



